\documentclass[a4paper,11pt,headings=small,twocolumn]{article}

\usepackage[T1]{fontenc}
\usepackage[pdftex]{graphicx}
\usepackage{dblfloatfix}
\usepackage{multicol}
\usepackage{cite}
\usepackage{pdfpages}
\usepackage{cite}
\usepackage{tikz}
\usepackage{multirow}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{mathcomp}
\graphicspath{{images/}}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fouridx}
\usepackage{cancel}	
\usepackage{setspace}
\usepackage[ansinew]{inputenc}
\usepackage[format=plain,font=small,margin=10pt,labelfont=bf,labelsep=quad]{caption}
\usepackage{subcaption}
\usepackage{ipa}
\usepackage{a4wide}
\usepackage{titlesec}
\usepackage{array}
\usepackage{booktabs}
\usepackage[top=3.0cm, bottom=3.0cm, left=2cm, right=2cm]{geometry}
\sloppy
\usepackage{fancyhdr}
\usepackage{verbatim}

\usepackage{hyperref}
\hypersetup{colorlinks=true}

\setlength{\columnsep}{1cm}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{comment}
NOTES/TODO

Lessons Learned:
* Covariance matrix is CRUCIAL. In fact, it's everything.
  * Should capture frames of the progression from bad to good kernel fn.
* Author was doing everything in pixels! This screws up my hyperparameters!
* How far into future to predict.
* How to get MAP (take the mean).
* Correspondence is key. In both cases, fixed major issues.
* Taking the mean to approximate the MAP: is this valid?

To Include:
* Homography transform
* Kernel & hyperparameter definition
	* Charts describing kernel
* Final MAP calculation (mean weighted by IP)
* Performance metric (path length + safety calculation)
* Final results (over how many frames? for different particle swarm sizes?)

Future:
* Do more experiments.
* Incorporate static obstacles.
* Learn how to learn my own hyperparameters.
	* Pete gave me an example script. I'd like to try learning hyperparameters for the world coordinate frame instead of doing everything in pixels.
* Tweaking of interaction potential parameters.
* Clever methods of assigning destinations to pedestrians.
* The algorithm presented here took about 2 seconds to plan a single path, with 100 particles. The original paper cites times of 0.1 seconds for the same number of particles, so there is much room for improvement here. The original paper was written in Matlab but I'm not sure the language of implementation is the bottleneck. More likely, the GPML Matlab toolbox's implementation of Gaussian processes is much more efficient than my own (some parts of the toolbox are written in C).
* Explore shared autonomy, or local vs. global path planning?
* Implement completely on-robot?

\end{comment}

\begin{document}

\twocolumn[
  \begin{@twocolumnfalse}
  \vspace{-1cm}
	\begin{flushright}
	April 10, 2014\\
	\end{flushright}
	\vspace{0.6cm}
	\LARGE{\textbf{Robot Navigation in Dense Human Crowds}\\[0.2cm] Final Report}\\\\ \\
	\large{Neil Traft\\[0.1cm] University of British Columbia}		 	 	\vspace{0.6cm}
\end{@twocolumnfalse}
]


\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Robot Navigation in Dense Human Crowds}
\fancyhead[R]{Neil Traft --- \thepage}

\pagenumbering{arabic}
\setcounter{page}{1}

%\onehalfspacing
\renewcommand{\thesection}{\Roman{section}}
\thispagestyle{empty}

\newcommand{\f}{\mathbf{f}}
\newcommand{\fr}{\f^{(R)}}
\newcommand{\fati}{\f^{(i)}}
\newcommand{\fatj}{\f^{(j)}}
\newcommand{\samplej}{(\fatj)_i}
\newcommand{\z}{\mathbf{z}}
\newcommand{\ztot}{\z_{1:t}}
\newcommand{\pos}{\mathrm{pos}}
\newcommand{\post}{\pos^{(i)}_t}

\section*{Overview}
\quad In their 2010 IROS publication \cite{Trautman2010}, Trautman and Krause develop a path planning algorithm that is safe and yet does not suffer from the ``freezing robot problem'' (FRP). Their method consists of a model of crowd interaction combined with a particle-based inference method to predict where the crowd (and the robot) should be at some time $t+1$ in the future. The idea is that if one can develop a reliable model of intelligent agents in a crowd, and include the robot as just another of those intelligent agents, then the predictions of the model yield the robot's future path.

The goal of this project is to reproduce their results in simulation on the original dataset. Given some annotated video of pedestrians in a crowd, we can choose one of the pedestrians to represent the robot, and compare the path planned by the robot to the actual path taken by the pedestrian.

\subsubsection*{Interacting Gaussian Processes}
\quad The crowd interaction model is a nonparametric stochastic model based on Gaussian processes, dubbed \emph{Interacting Gaussian Processes} (IGP). In IGP, the actions of all agents, including the robot, are modeled as a joint distribution:
$$p(\fr,\f|\ztot)$$
where $\fr$ is the robot's trajectory over $T$ timesteps, $\f$ is the set of all human trajectories, and $\ztot$ is the set of all observations up to the current time point. For the purposes of this algorithm, observations of human and robot position are taken to be more or less perfect, since we are only trying to solve the navigation problem, not situational awareness.

At each timestep, each agent's new position is represented as a random variable from a probability distribution. It is important to note that this distribution is \emph{not} Gaussian, due to two major additions to avoid the uncertainty explosion which leads to the FRP (see Figure \ref{uncertainty}).

First, goal information is given as a final ``observation'' at time $T$, resulting in the full set of observations $\z_{1:t,T}$. The robot's goal, $y_T^{(R)}$, is known and can be added with good confidence. The goals of other agents can be omitted or can be added with a high variance, to encode how uncertain we are about the goal.

The second addition IGP makes to standard Gaussian processes is the inclusion of an ``interaction potential'',
\begin{align*}
\psi(\fr,\f) &= \\
\prod_{i=1}^n \prod_{j=i+1}^n \prod_{\tau=t}^T & \left( 1 - \alpha \exp\big(-\frac{1}{2h^2}|\fati_{\tau} - \fatj_{\tau}|\big) \right)
\end{align*}

In essence, this potential grows very small whenever two agents $i$ and $j$ become very close at any time $\tau$. This has the result that any set of paths where agents become too close is treated as very unlikely. The parameter $h$ controls the desired ``safety distance'' and $\alpha \in [0, 1]$ controls the ``repelling force''. Thus, the final posterior is given as:
$$ p_{IGP}(\fr,\f | \ztot) = \frac{1}{Z} \; \psi(\fr,\f) \prod_{i=1}^n p(\fati | \ztot) $$

% What is Z?
% Include image of IP?

The above is a nonlinear, multimodal distribution, so it can't be sampled directly. Instead, we sample from Gaussian priors $p(\fati|\ztot)$ and resample weighted by our desired distribution (a particle filter). This is described in the next section.

\begin{figure*}[h]
	\centering
	\includegraphics[width=.75\textwidth]{uncertainty-explosion.png}
	\caption{Diagrams taken from \cite{Trautman2010} describing the uncertainty explosion which leads to the Freezing Robot Problem. \textbf{Left:} Depicts the uncertainty explosion in standard motion models, where each agent's trajectory is \emph{independent} from the others. \textbf{Middle:} A demonstration of why even \emph{perfect} prediction, devoid of uncertainty, can still lead to the FRP. In crowded environments, \emph{all} paths can have a high cost function, leading to extreme evasive maneuvers or freezing. \textbf{Right:} The ideal model, based on the insight that intelligent agents engage in \emph{cooperative} collision avoidance.}
	\label{uncertainty}
\end{figure*}

\subsubsection*{Importance Sampling}
\quad Now that we have a model, we wish to sample from it and take the mean as the desired path. Since we can't sample from it directly, we instead use the \emph{importance sampling} technique which is widely used in particle filters. Each sample is weighted by the ratio of the IGP to the basic GP (i.e. the Gaussian distribution, without the interaction potential):
$$ w_i = \frac{p_{IGP}}{p_{GP}} = \frac{p_{IGP}((\fr,\f) | \ztot)}{\prod_{j=R}^n p(\samplej | \ztot)} = \frac{\psi((\fr,\f)_i) \prod_{j=R}^n p(\samplej | \ztot)}{\prod_{j=R}^n p(\samplej | \ztot)} = \psi((\fatj)_i)$$
where $\samplej$ is a single sample from the trajectory of agent $j$.

Given this formulation for $p_{IGP}$ and an appropriate weighting $w_i$ for each sample, the ideal paths can now be expressed as:
$$ (\fr,\f)^* = \argmax\left( \sum_{i=1}^N w_i(\fr,\f)_i \right) $$
where $(\fr,\f)_i$ is a set of samples from the Gaussian processes $(\fatj)_i \sim \textrm{GP}(\fatj, m_t^{(j)}, k_t^{(j)})$ (detailed equations for the mean and covariance matrix can be found in the original paper). The total number of samples is $N$ and we take the robot's next position to be $\fr_{t+1}$. To approximate the optimal robot path $\fr$, we take the mean path over all samples after importance resampling:
$$ \mathrm{f}^{(R)*}_t = \frac{\sum_{i=1}^N \mathrm{f}^{(R)}_t}{N} $$

\section*{Implementation}
\quad The project is implemented in Python, using the OpenCV library\cite{opencv} for visualization and video playback (but not for any of the vision algorithms it provides).

\subsection*{The Dataset}
\quad The dataset to be used is the ETH Walking Pedestrians (EWAP) dataset from \cite{Pellegrini2009}. It can be obtained from \cite{dataset}.

The dataset contains two annotated videos of pedestrian interaction captured from a bird's eye view. The one used depicts pedestrians entering and exiting a building.

The main annotations are a matrix where each row has the format:
\begin{verbatim}
        [frame_number pedestrian_ID pos_x pos_z pos_y v_x v_z v_y]
\end{verbatim}

Thus for each frame $t$ we have potentially multiple pedestrian observations $\post$, and this forms our observation at time $t$:
$$ \z_t = \pos^{(1:n)}_t $$
where one of the $n$ pedestrians is chosen to represent the robot $R$. The velocities are not used in the present IGP formulation.

The positions and velocities are in meters and were obtained with a homography matrix $H$, which is also provided with the annotations. To transform the positions back to image coordinates, it is necessary to apply the inverse homography transform:
$$ \fourIdx{m}{}{}{}\post = H_{mw}^{-1} \cdot \fourIdx{w}{}{}{}\post \qquad m = \mathrm{image}, w = \mathrm{world} $$
The intrinsic camera parameters are not needed; it is presumed that they are included in the camera matrix provided by the dataset. There is also no translation needed; the origin of the world can be taken to be the (transformed) origin of the image without loss of generality. Thus, pixel coordinates can be expressed as
$$ r = -f_x \frac{x}{z} = \frac{u}{z} \qquad c = -f_y \frac{y}{z} = \frac{v}{z} $$
where $f_x,\;f_y$ are the intrinsic camera parameters and $x,\;y$ are coordinates in the image frame. So to obtain pixel coordinates from image coordinates it is necessary only to normalize so that $z=1$:
$$ \begin{pmatrix}r\\c\\1\end{pmatrix} = \begin{pmatrix}\fourIdx{m}{}{}{}x/z\\\fourIdx{m}{}{}{}y/z\\\fourIdx{m}{}{}{}z/z\end{pmatrix} $$

\subsection*{Technical Hurdles}

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.8\textwidth}
	\centering
	\includegraphics[width=\textwidth]{SE+noise-prior.png}
	\caption{A summed kernel composed of squared exponential and noise kernels.}
	\label{a}
	\end{subfigure}
	\begin{subfigure}[b]{0.8\textwidth}
	\centering
	\includegraphics[width=\textwidth]{matern+lin+noise-prior.png}
	\caption{A summed kernel composed of Matérn, linear regression, and noise kernels.}
	\label{b}
	\end{subfigure}
	\caption{The importance of choosing a proper covariance function. (\ref{a}) This is much too curvy and loopy to represent a human path. Humans generally have destinations; they don't wander aimlessly. (\ref{b}) We consider this to be a much better model of human movement. Human paths are mostly straight, but with occasional curviness.}
	\label{cov}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.8\textwidth}
	\centering
	\includegraphics[width=\textwidth]{bad-cov-bad-hp.png}
	\caption{Poor choice of covariance function (squared exponential) and poorly tuned hyperparameters.}
	\label{a}
	\end{subfigure}
	\begin{subfigure}[b]{0.8\textwidth}
	\centering
	\includegraphics[width=\textwidth]{bad-cov-good-hp.png}
	\caption{Improved hyperparameters are still not enough to overcome a fundamentally incorrect covariance function.}
	\label{b}
	\end{subfigure}
	\begin{subfigure}[b]{0.8\textwidth}
	\centering
	\includegraphics[width=\textwidth]{good-cov-bad-hp.png}
	\caption{Good choice of covariance function (Matérn + linear + noise) brings us much closer to the mark, but badly tuned hyperparameters still makes things difficult.}
	\label{c}
	\end{subfigure}
	\begin{subfigure}[b]{0.8\textwidth}
	\centering
	\includegraphics[width=\textwidth]{good-cov-good-hp.png}
	\caption{Good covariance function combined with good hyperparameters.}
	\label{d}
	\end{subfigure}
	\caption{The importance of choosing proper hyperparameters. Hyperparameters must be learned to properly match what we expect from human and robot movement patterns. Hyperparameters must be further tuned for a particular scene, since they are closely related to the scale of our coordinate frame.}
	\label{hp}
\end{figure}

\section*{Experimental Results}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{experiment.png}
	\caption{The results of the algorithm performed on 12 pedestrians from a particularly crowded segment of the video. Observe that IGP almost always finds an equal length or shorter path than the pedestrian. However, the pedestrian usually maintains a larger distance from other agents, and IGP may be slightly unsafe based on these results.}
	\label{experiment}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{sample-results.png}
	\caption{The results reported in the original paper, compiled from 10 trials. Here, we see that IGP outperforms human pedestrians in both path length and path safety. We aren't told which 10 pedestrians are evaluated in the original paper, so these results are somewhat anecdotal and will not necessarily align with our own.}
	\label{sample}
\end{figure}


\section*{Next Steps}


\bibliography{references}
\bibliographystyle{ieeetr}

\end{document}
